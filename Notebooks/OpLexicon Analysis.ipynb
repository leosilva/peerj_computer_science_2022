{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "760f988f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/leosilva/opt/anaconda3/lib/python3.7/site-packages (3.0.5)\n",
      "Requirement already satisfied: setuptools in /Users/leosilva/opt/anaconda3/lib/python3.7/site-packages (from spacy) (52.0.0.post20210125)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/leosilva/opt/anaconda3/lib/python3.7/site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /Users/leosilva/opt/anaconda3/lib/python3.7/site-packages (from spacy) (8.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/leosilva/opt/anaconda3/lib/python3.7/site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /Users/leosilva/opt/anaconda3/lib/python3.7/site-packages (from spacy) (2.4.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /Users/leosilva/opt/anaconda3/lib/python3.7/site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /Users/leosilva/opt/anaconda3/lib/python3.7/site-packages (from spacy) (3.7.4.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/leosilva/opt/anaconda3/lib/python3.7/site-packages (from spacy) (0.7.4)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /Users/leosilva/opt/anaconda3/lib/python3.7/site-packages (from spacy) (3.7.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/leosilva/opt/anaconda3/lib/python3.7/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/leosilva/opt/anaconda3/lib/python3.7/site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/leosilva/opt/anaconda3/lib/python3.7/site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/leosilva/opt/anaconda3/lib/python3.7/site-packages (from spacy) (1.19.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/leosilva/opt/anaconda3/lib/python3.7/site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/leosilva/opt/anaconda3/lib/python3.7/site-packages (from spacy) (4.59.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /Users/leosilva/opt/anaconda3/lib/python3.7/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/leosilva/opt/anaconda3/lib/python3.7/site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /Users/leosilva/opt/anaconda3/lib/python3.7/site-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /Users/leosilva/opt/anaconda3/lib/python3.7/site-packages (from spacy) (1.7.3)\n",
      "Requirement already satisfied: jinja2 in /Users/leosilva/opt/anaconda3/lib/python3.7/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/leosilva/opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20->spacy) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/leosilva/opt/anaconda3/lib/python3.7/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /Users/leosilva/opt/anaconda3/lib/python3.7/site-packages (from pathy>=0.3.5->spacy) (3.0.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/leosilva/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/leosilva/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/leosilva/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/leosilva/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /Users/leosilva/opt/anaconda3/lib/python3.7/site-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/leosilva/opt/anaconda3/lib/python3.7/site-packages (from jinja2->spacy) (1.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/leosilva/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to /Users/leosilva/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import nltk\n",
    "import spacy\n",
    "from spacy.lang.pt import Portuguese\n",
    "nltk.download('stopwords')\n",
    "nltk.download('rslp')\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dbaa3aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "oplexicon = open('/Users/leosilva/development/thesis/Lexicos/oplexicon_v3.0/lexico_v3.0.txt','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "60d53aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = Portuguese()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9a2e981a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_str_twitter</th>\n",
       "      <td>23781146</td>\n",
       "      <td>15148309</td>\n",
       "      <td>927028057763995648</td>\n",
       "      <td>77648005</td>\n",
       "      <td>3429765093</td>\n",
       "      <td>10794662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>Fe Rebelatto</td>\n",
       "      <td>Sommelier de Airbnb</td>\n",
       "      <td>✪ Felippe</td>\n",
       "      <td>Conference Distinguished Specialist</td>\n",
       "      <td>Paula Santana</td>\n",
       "      <td>Maharaja of the Legacies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screen_name</th>\n",
       "      <td>rebelatto</td>\n",
       "      <td>Guilh_rm_</td>\n",
       "      <td>FelippeRegazio</td>\n",
       "      <td>NannoKa</td>\n",
       "      <td>psanrosa13</td>\n",
       "      <td>rponte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>somewhere in Brazil</td>\n",
       "      <td></td>\n",
       "      <td>Ctba-PR Brasil</td>\n",
       "      <td>Brasil</td>\n",
       "      <td>Ceará, Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>url</th>\n",
       "      <td>None</td>\n",
       "      <td>https://t.co/gX4oRZWHg3</td>\n",
       "      <td>https://t.co/KkaPlVH4Wu</td>\n",
       "      <td>https://t.co/hDni4Fh2Ng</td>\n",
       "      <td>https://t.co/YJQuDtv2YW</td>\n",
       "      <td>https://t.co/3yZ94QNQvs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    26                       27  \\\n",
       "id_str_twitter                23781146                 15148309   \n",
       "name                      Fe Rebelatto      Sommelier de Airbnb   \n",
       "screen_name                  rebelatto                Guilh_rm_   \n",
       "location        Rio de Janeiro, Brasil      somewhere in Brazil   \n",
       "url                               None  https://t.co/gX4oRZWHg3   \n",
       "\n",
       "                                     28                                   29  \\\n",
       "id_str_twitter       927028057763995648                             77648005   \n",
       "name                          ✪ Felippe  Conference Distinguished Specialist   \n",
       "screen_name              FelippeRegazio                              NannoKa   \n",
       "location                                                      Ctba-PR Brasil   \n",
       "url             https://t.co/KkaPlVH4Wu              https://t.co/hDni4Fh2Ng   \n",
       "\n",
       "                                     30                        31  \n",
       "id_str_twitter               3429765093                  10794662  \n",
       "name                      Paula Santana  Maharaja of the Legacies  \n",
       "screen_name                  psanrosa13                    rponte  \n",
       "location                         Brasil             Ceará, Brazil  \n",
       "url             https://t.co/YJQuDtv2YW   https://t.co/3yZ94QNQvs  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json(r'/Users/leosilva/development/thesis/workspacePyCharm/TwitterDataMining/alldata.json')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "362fe3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32119\n"
     ]
    }
   ],
   "source": [
    "dic_palavra_polaridade = {}\n",
    "\n",
    "for i in oplexicon.readlines():\n",
    "    linha_splitted = i.split(',')\n",
    "    palavra = linha_splitted[0]\n",
    "    polaridade = linha_splitted[2]\n",
    "    dic_palavra_polaridade['{}'.format(palavra)] = polaridade\n",
    "\n",
    "print(len(dic_palavra_polaridade))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3dc600e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentimento(frase):\n",
    "    frase = frase.lower()\n",
    "    l_sentimento = []\n",
    "    for p in frase.split():\n",
    "        l_sentimento.append(int(dic_palavra_polaridade.get(p, 0)))\n",
    "    score = sum(l_sentimento)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c6048210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_sentimento('Estou Muito Feliz hoje,super animado com o trabalho novo! :)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7eed40c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_tweets_per_user = {}\n",
    "\n",
    "for c in data:\n",
    "    id_user = c\n",
    "    tweets = []\n",
    "    for t in data[id_user]['tweets']:\n",
    "        tweets.append({\"id\": t[0], \"id_str_twitter\": t[1], \"text\": t[2], \"oplexicon\": None})\n",
    "\n",
    "        dic_tweets_per_user['{}'.format(id_user)] = tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7bff15e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the tweets\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "23986089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(tweet_text):\n",
    "    # remove twitter Return handles (RT @xxx:)\n",
    "    tweet_text = np.vectorize(remove_pattern)(tweet_text, \"RT @[\\w]*:\")\n",
    "\n",
    "    # remove twitter handles (@xxx)\n",
    "    tweet_text = np.vectorize(remove_pattern)(tweet_text, \"@[\\w]*\")\n",
    "    \n",
    "#     tweet_text = re.sub(r\"http\\S+\", \"\", tweet_text).lower()\n",
    "#     tweet_text = re.sub(r\"[-|0-9]\", \"\", tweet_text).lower()\n",
    "#     tweet_text = re.sub(r'[-./?!,\":;()\\']', ' ', tweet_text).lower()\n",
    "    \n",
    "    # colocar para funcionar o seguinte algoritmo:\n",
    "    \n",
    "    # remove URL links (httpxxx)\n",
    "    tweet_text = np.vectorize(remove_pattern)(tweet_text, \"https?://[A-Za-z0-9./]*\")\n",
    "\n",
    "    # remove special characters, numbers, punctuations (except for #)\n",
    "    tweet_text = np.core.defchararray.replace(tweet_text, \"[^a-zA-Z]\", \" \")\n",
    "    \n",
    "    tweet_text = np.array2string(tweet_text)\n",
    "    \n",
    "    return tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c5f28a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('portuguese')\n",
    "\n",
    "pontuacao = list()\n",
    "for ponto in punctuation:\n",
    "    pontuacao.append(ponto)\n",
    "\n",
    "def remove_stop_words(tweet_text): # Função para remover stopwords e pontuações\n",
    "    tweet_text = nlp(tweet_text)\n",
    "    palavras = [palavra.text for palavra in tweet_text if palavra.text not in stop_words and palavra.text not in pontuacao]\n",
    "    return \" \".join(palavras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "59228e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeated_letters(tweet_text): # Função para remover letras repetidas\n",
    "    tweet_text = nlp(tweet_text)\n",
    "    palavras = list()\n",
    "    for palavra in tweet_text:\n",
    "        if palavra.text not in list(dic_palavra_polaridade.keys()):\n",
    "            palavras.append(re.compile(r'(.)\\1{1,}', re.IGNORECASE).sub(r'\\1', palavra.text))\n",
    "        else:\n",
    "            palavras.append(palavra.text)\n",
    "    return (\" \".join(palavras))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dfd27057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(tweet_text):\n",
    "    stemmer = RSLPStemmer()\n",
    "    tweet_splitted = tweet_text.split(' ')\n",
    "    phrase = []\n",
    "    for word in tweet_splitted:\n",
    "        if word is not '':\n",
    "            phrase.append(stemmer.stem(word.lower()))\n",
    "    return (\" \".join(phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "918a8a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(score, alpha=15):\n",
    "    \"\"\"\n",
    "    Normalize the score to be between -1 and 1 using an alpha that\n",
    "    approximates the max expected value\n",
    "    \"\"\"\n",
    "    norm_score = score/math.sqrt((score*score) + alpha)\n",
    "    return norm_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "763a1fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positivos: 9377\n",
      "Negativos: 5253\n",
      "Neutros: 10976\n",
      "\n",
      "\n",
      "Pos Score: 2\n",
      "Norm Pos Score: 0\n",
      "Pos Score: 2\n",
      "Norm Pos Score: 0\n",
      "Pos Score: 2\n",
      "Norm Pos Score: 0\n",
      "Pos Score: 1\n",
      "Norm Pos Score: 0\n",
      "Pos Score: 1\n",
      "Norm Pos Score: 0\n",
      "\n",
      "\n",
      "Neg Score: -1\n",
      "Norm Neg Score: 0\n",
      "Neg Score: -1\n",
      "Norm Neg Score: 0\n",
      "Neg Score: -1\n",
      "Norm Neg Score: 0\n",
      "Neg Score: -2\n",
      "Norm Neg Score: 0\n",
      "Neg Score: -3\n",
      "Norm Neg Score: -1\n"
     ]
    }
   ],
   "source": [
    "pos_scores = []\n",
    "norm_pos_scores = []\n",
    "neg_scores = []\n",
    "norm_neg_scores = []\n",
    "neu_scores = []\n",
    "\n",
    "for u in dic_tweets_per_user:\n",
    "    tweets = dic_tweets_per_user[u]\n",
    "    for t in tweets:\n",
    "        tweet = t['text']\n",
    "        tweet = clean_data(tweet)\n",
    "        tweet = remove_stop_words(tweet)\n",
    "        tweet = remove_repeated_letters(tweet)\n",
    "        #tweet = stemming(tweet)\n",
    "        oplexicon_analysis = score_sentimento(tweet)\n",
    "        if oplexicon_analysis > 0:\n",
    "            pos_scores.append(oplexicon_analysis)\n",
    "            oplexicon_analysis = round(normalize(oplexicon_analysis))\n",
    "            norm_pos_scores.append(oplexicon_analysis)\n",
    "        elif oplexicon_analysis == 0:\n",
    "            neu_scores.append(oplexicon_analysis)\n",
    "        else:\n",
    "            neg_scores.append(oplexicon_analysis)\n",
    "            oplexicon_analysis = round(normalize(oplexicon_analysis))\n",
    "            norm_neg_scores.append(oplexicon_analysis)\n",
    "            \n",
    "print('Positivos: {}'.format(len(pos_scores)))\n",
    "print('Negativos: {}'.format(len(neg_scores)))\n",
    "print('Neutros: {}'.format(len(neu_scores)))\n",
    "\n",
    "print('\\n')\n",
    "print('Pos Score: {}'.format(pos_scores[0]))\n",
    "print('Norm Pos Score: {}'.format(norm_pos_scores[0]))\n",
    "print('Pos Score: {}'.format(pos_scores[1]))\n",
    "print('Norm Pos Score: {}'.format(norm_pos_scores[1]))\n",
    "print('Pos Score: {}'.format(pos_scores[2]))\n",
    "print('Norm Pos Score: {}'.format(norm_pos_scores[2]))\n",
    "print('Pos Score: {}'.format(pos_scores[3]))\n",
    "print('Norm Pos Score: {}'.format(norm_pos_scores[3]))\n",
    "print('Pos Score: {}'.format(pos_scores[4]))\n",
    "print('Norm Pos Score: {}'.format(norm_pos_scores[4]))\n",
    "\n",
    "print('\\n')\n",
    "print('Neg Score: {}'.format(neg_scores[0]))\n",
    "print('Norm Neg Score: {}'.format(norm_neg_scores[0]))\n",
    "print('Neg Score: {}'.format(neg_scores[1]))\n",
    "print('Norm Neg Score: {}'.format(norm_neg_scores[1]))\n",
    "print('Neg Score: {}'.format(neg_scores[2]))\n",
    "print('Norm Neg Score: {}'.format(norm_neg_scores[2]))\n",
    "print('Neg Score: {}'.format(neg_scores[3]))\n",
    "print('Norm Neg Score: {}'.format(norm_neg_scores[3]))\n",
    "print('Neg Score: {}'.format(neg_scores[4]))\n",
    "print('Norm Neg Score: {}'.format(norm_neg_scores[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db85958c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd188d08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
